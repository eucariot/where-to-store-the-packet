
<!DOCTYPE html>

<html lang="ru">
  <head>
    <meta charset="utf-8" />
    <title>Перегрузки: причины и места &#8212; документация Where to store packets 1.0</title>
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/translations.js"></script>
    <link rel="index" title="Алфавитный указатель" href="../genindex.html" />
    <link rel="search" title="Поиск" href="../search.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="id1">
<h1>Перегрузки: причины и места<a class="headerlink" href="#id1" title="Ссылка на этот заголовок">¶</a></h1>
<p>Буфер нужен на сетевом устройстве не только для того, чтобы похранить тело пакета, пока его заголовки перевариваются в кишках чипа.
Как минимум нужно сгладить поток пакетов до скорости выходного интерфейса. Грубо говоря, пока один пакет сериализуется для передачи в интерфейс, второму нужно где-то подождать.
Для этой задачи обычно хватит совсем небольшой FIFO очереди.
Другая важнейшая сетевая задача - перегрузки (congestion). Если в один интерфейс одновременно сваливается много пакетов, их тоже нужно сохранить.</p>
<p>Причём перегрузки - это наша повседневная легитимная реальность, а не что-то крайне редкое, что нужно перетерпеть и станет попроще.
Во времена коммутации каналов такой проблемы не стояло - для любой общающейся пары всегда было зарезервировано строго необходимое количество ресурсов.
Сегодня совершенно законно на порт может прийти больше трафика, чем тот готов сиюминутно пропустить.</p>
<div class="section" id="id2">
<h2>Причины перегрузок<a class="headerlink" href="#id2" title="Ссылка на этот заголовок">¶</a></h2>
<p>Самая простая - из высокоскоростного интерфейса трафик должен слиться в более низкоскоростной - из <strong>10G в 1</strong>, например.
Другая причина, очень распространённая в сетях крупных ДЦ, особенно в тех, где развёрнуты кластеры Map-Reduce - это <strong>Incast</strong>. Это ситуация, в которой одна машина отправляет запрос на десятки/сотни/тысячи, а те все разом начинают отвечать, и пакеты стопорятся на узком интерфейсе в сторону машины-инициатора.
Более общий случай - трафик с нескольких входящих портов должен влиться в один исходящий - <strong>Backpressure</strong>.
Прочие типы всплесков трафика, которые ещё называют бёрстовыми или просто бёрстами (<strong>Bursts</strong>).</p>
<p>Поэтому однозначно нужно побольше памяти для буферизации. Но фактически это место, где пакеты обрастают задержкой.
Так на заре 1G буферизация вызывала массу головной боли у трейдеров, чьи приложения получали свой бесценный трафик с задержкой и джиттером.</p>
<p>И поэтому тут уже недостаточно везде FIFO. Это задача, во благо которой трудится <a class="reference external" href="https://linkmeup.ru/blog/365.html">QoS</a>.</p>
<p>Если случилась конгестия, то нужно иметь возможность требовательный к задержкам трафик, пропустить первым, чувствительный к потерям - не дропнуть, а наименее ценным пожертвовать.
Ещё нужно уметь пополисить и пошейпить.</p>
<p>Но в каком месте располагать эту память и где реализовывать QoS?</p>
</div>
<div class="section" id="id3">
<h2>Места возникновения перегрузок<a class="headerlink" href="#id3" title="Ссылка на этот заголовок">¶</a></h2>
<p>Их по большому счёту 4:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><strong>на входном чипе</strong> - если со стороны интерфейсов на него поступает больше, чем он способен обработать.</p></li>
<li><p><strong>на фабрике коммутации</strong> (если коробка модульная) - если линейные карты пытаются отправить на фабрику больше, чем она способна обработать.</p></li>
<li><p><strong>на выходной линейной карте</strong>, если фабрика пытается передать на линейную карту больше, чем её чип способен обработать</p></li>
<li><p><strong>на выходном интерфейсе</strong> - если чип шлёт в интерфейс больше, чем тот способен сериализовать.</p></li>
</ol>
<div class="figure align-center">
<a class="reference internal image-reference" href="https://fs.linkmeup.ru/images/articles/buffers/congestion_points.svg"><img alt="https://fs.linkmeup.ru/images/articles/buffers/congestion_points.svg" src="https://fs.linkmeup.ru/images/articles/buffers/congestion_points.svg" width="600" /></a>
</div>
</div></blockquote>
<p>Но никто не хочет бороться с перегрузками в четырёх местах.
Поэтому обычно
А) Чип делают такой производительности, чтобы он смог обработать весь трафик, даже если тот начал одновременно поступать со всех портов на этой линейной карте. Поэтому для устройства со 128 портами 100Гб/с используется чип с производительностью 12,8Тб/с.
Очевидно бывают и исключения. Тогда или имеем непредсказуемые потери, или (чаще) невозможность использовать часть портов.
Б) Фабрику так же делают без переподписки, чтобы она могла провернуть весь трафик, который пытаются в неё передать все линейные карты, даже если они делают это одновременно на полной скорости. Таким образом не нужно буферизировать трафик и перед отправкой на фабрику. Более того, её приходится делать не просто равной сумме пропускных способностей входных/выходных интерфейсов, а больше - там ведь помимо самих пакетов передаются ещё метаданные.
В) Управление перегрузками на выходном чипе и выходном интерфейсе сводят в одно место.</p>
<blockquote>
<div><p>На самом деле фабрика без передподиски (или неблокируемая) - это та ещё спекуляция, к которой нередко прибегают маркетологи.
Для некоторых сценариев, например, CIOQ даже со speedup фабрики в пару раз от необходимого есть строгие результаты, показывающие, при каких условиях она будет неблокируемой.
Можно почитать у достопочтенных выпускников MIT и Стэнфорда: <a class="reference external" href="http://yuba.stanford.edu/~nickm/papers/CSL-TR-97-738.pdf">On the speedup required for combined input and output queued switching</a>.</p>
</div></blockquote>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">Where to store packets</a></h1>








<h3>Навигация</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Быстрый поиск</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Искать" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020, eucariot.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.0.3</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/5_buffers/2_congestions.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>