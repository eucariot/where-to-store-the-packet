Чипы для датацентровых коммутаторов
===================================

Чтобы упростить себе жизнь, я продолжу далее разговор только об ASIC'ах под датацентровые коммутаторы, не пытаясь обнять Джабба Хатта.

До недавних пор на этой ниве пахал только Broadcom со своей оружейной палатой: Tomahawk и Trident - и израильскими городами: Qumran, Jericho итд.
Выбор - особо не разбежишься - ну или разрабатывать своё (как делают Huawei, Juniper и Cisco)

Сегодня конкуренцию ему пытаются составить Mellanox со своими собственными чипами Spectrum (ныне уже Nvidia), Innovium Teralynx, Barefoot Tophino (ныне Intel). Своим появлением эти компании раскачивают рынок и провоцируют среди вендоров тренд на переход от внутренних разработок к готовым чипам их производства.

Мы в конце книги взглянем на их модельные ряды, но пока давайте обсудим, чем же чипы характеризуются и могут отличаться друг от друга.

А для этого надо понять, какие они задачи решают.

В `датацентровых сетях <https://linkmeup.ru/blog/480.html>`_ есть три основных типа устройств:

**Spine** - сравнительно простая железка, требующая самый минимум функций - её задача просто молотить трафик. Очень много и очень быстро. Зачастую это просто IP-маршрутизация. Но бывают и топологии, в которых Spine играет чуть более важную роль (VXLAN anycast gateway). Но обычная практика - держать конфигурацию спайнов максимально простой.

**Leaf** - чуть более требователен к функциям. Может терминировать на себе VXLAN или другие оверлеи. Здесь могут реализовываться политики QoS и ACL. Зато не нужна такая большая пропускная способность, как для спайнов. Кроме того, в некоторых сценариях (VXLAN) leaf знает о сервисах за подключенными машинами (клиентских сетях, контейнерах), соответственно, ему нужно больше ресурсов FIB для хранения этой инфорации.

**Edge-leaf** - это устройства границы сети ДЦ и здесь уже фантазия ограничивается только свободой мысли сетевых архитекторов - MPLS, RSVP-TE, Segment Routing, всевозможные VPN'ы. При этом наименее требовательны к производительности.

На каждом устройстве, соответственно, разные требования к возможностям чипов - как по пропускной способности, так и по набору функций и по количеству ресурсов для хранения чего-либо.

И надо сказать, вендоры чипов и железа добились тут поразительных успехов. 
Типичный спайн сегодня - это 64-128 100GE портов на 2-4 юнита с энергопотреблением около 400 Вт. И ценой порядка пары десятков тысяч долларов.

    .. figure:: https://fs.linkmeup.ru/images/articles/buffers/nexus3k.png           
           :width: 800
           :align: center

Производителям чипов приходится нелегко не только из-за попыток найти золотую середину, но и из-за возрастающих скоростей передачи данных и конкуренции.
Средняя скорость аплинков с торов сегодня 200-800 Гб/с. Чтобы собрать минимально рабочую сеть ДЦ, нужны спайны с пропускной способностью 3,2 Тб/с.

Всё более и более производительные чипы нужно выпускать уже примерно каждые полтора-два года.

    .. figure:: https://fs.linkmeup.ru/images/articles/buffers/timeline.png           
           :width: 800
           :align: center

*Актуализированная мной картинка из `видео PP <https://youtu.be/Ti3t9OAZL3g?t=2496>`_.*


Конкурирующие производители чипов идут ноздря в ноздрю - почти одновременно у всех (Broadcom, Mellanox, Innovium, Barefoot) выходят микросхемы с почти идентичными характеристиками, а вслед за ними и коммутаторы с ними.

Ещё одним компромиссным вопросом является размер буфера, но об этом мы поговорим ` <#BUFERA">попозже>`_.


Помимо скорости и обязательных функций по маршрутизации и оверлеям, есть ещё много менее заметных вещей, которые ожидают потребители. 
Мы про них говорить сегодня не будем, но не упомянуть было бы ошибкой.

Это, например, **телеметрия** в реальном времени: наблюдать за утилизацией буферов, видеть бёрсты, дампы отброшенных пакетов, профиль трафика по размерам и типам пакетов.
Кроме того, сегодня набирает популярность **INT** - `Inband Network Telemetry <https://www.opencompute.org/files/INT-In-Band-Network-Telemetry-A-Powerful-Analytics-Framework-for-your-Data-Center-OCP-Final3.pdf>`_.

Для многих незаметно, но уже почти жизненно важно, начинает работать **динамическая балансировка трафика**: чип отслеживает потоки (flows) и дробит их на флоулеты (flowlets) - короткие куски трафика одного потока, разделённые между собой паузой в несколько миллисекунд. Эти флоулеты он может динамически распределять по разным путям (ECMP или членам LAG), чтобы обеспечить более равномерную балансировку. Особенно важно это для Elephant Flows, оккупирующих один интерфейс.

Пользователям всё чаще хочется иметь возможность **управлять распределением буфера**, ну а **перераспределение** ресурсов FIB - это уже функциональность, отсутствие которой будет вызывать вопросы. 

В условиях датацентров `ECMP и балансировка силами сети <https://linkmeup.ru/blog/482.html>`_ - это воздух, вендорам нужно обеспечить нужное количество как **ECMP-групп**, так и общее **количество Next-hop'ов**.

Поэтому нет одного чипа или тем более SoC, решающего сразу все задачи. 
Под каждую роль разработаны свои чипы. Одни из них ориентированы на пропускную способность, другие на широкую функциональность, третьи на низкие задержки. 

Посочувствуем же бедным вендорам и будем выбирать долларом.